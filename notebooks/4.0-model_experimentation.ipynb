{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.0 - Model Experimentation\n",
        "\n",
        "Goal: Evaluate multiple classifiers across engineered feature sets to select the best combination for breast cancer diagnosis.\n",
        "\n",
        "- Primary metric: Recall \n",
        "- Secondary metrics: F1-score, Precision, Accuracy and ROC-AUC\n",
        "- Feature sets: `X_processed`, `X_pca`, `X_with_ratios`, `X_poly`\n",
        "- Models: Logistic Regression, Linear SVM, RBF SVM, Random Forest, XGBoost, KNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import and setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports & setup\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    RocCurveDisplay,\n",
        "    PrecisionRecallDisplay,\n",
        ")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Project imports\n",
        "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
        "from model.data_ingestion import load_raw_data\n",
        "from model.data_preprocessing import drop_unnecessary_columns, map_diagnosis_to_numerical, prepare_features_and_target\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll recreate the feature sets by re-running the preprocessing and feature engineering pipelines that were created on the previous notebook `3.0-feature_engineering.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate feature sets: X_processed, X_pca, X_with_ratios, X_poly, and y\n",
        "\n",
        "# Load data\n",
        "DATA_PATH = os.path.abspath(os.path.join('..', 'data', 'data.csv'))\n",
        "df_raw = load_raw_data(DATA_PATH)\n",
        "\n",
        "# Initial cleaning & mapping\n",
        "df_cleaned = drop_unnecessary_columns(df_raw.copy())\n",
        "df_mapped = map_diagnosis_to_numerical(df_cleaned.copy())\n",
        "\n",
        "# Unscaled features & target (keep unscaled copy for ratios)\n",
        "X_unscaled, y = prepare_features_and_target(df_mapped)\n",
        "\n",
        "# Baseline selection (match previous notebook)\n",
        "features_to_drop = [\n",
        "    'fractal_dimension_se',\n",
        "    'smoothness_se',\n",
        "    'fractal_dimension_mean',\n",
        "    'texture_se',\n",
        "    'symmetry_se'\n",
        "]\n",
        "X_selected_unscaled = X_unscaled.drop(columns=features_to_drop)\n",
        "\n",
        "# 1. Scaling -> X_processed\n",
        "scaler = StandardScaler()\n",
        "X_processed = pd.DataFrame(\n",
        "    scaler.fit_transform(X_selected_unscaled),\n",
        "    columns=X_selected_unscaled.columns\n",
        ")\n",
        "print('X_processed:', X_processed.shape)\n",
        "\n",
        "# 2. PCA -> X_pca (n_components selected to ~95% variance in 3.0; using 8)\n",
        "pca = PCA(n_components=8, random_state=SEED)\n",
        "X_pca = pd.DataFrame(\n",
        "    pca.fit_transform(X_processed),\n",
        "    columns=[f'PC_{i+1}' for i in range(8)]\n",
        ")\n",
        "print('X_pca:', X_pca.shape)\n",
        "\n",
        "# 3. Ratio features on unscaled, then scale -> X_with_ratios\n",
        "X_with_ratios_unscaled = X_selected_unscaled.copy()\n",
        "X_with_ratios_unscaled['shape_factor'] = (X_unscaled['perimeter_mean'] ** 2) / X_unscaled['area_mean']\n",
        "X_with_ratios_unscaled['radius_growth'] = X_unscaled['radius_worst'] / X_unscaled['radius_mean']\n",
        "\n",
        "scaler_ratios = StandardScaler()\n",
        "X_with_ratios = pd.DataFrame(\n",
        "    scaler_ratios.fit_transform(X_with_ratios_unscaled),\n",
        "    columns=X_with_ratios_unscaled.columns\n",
        ")\n",
        "print('X_with_ratios:', X_with_ratios.shape)\n",
        "\n",
        "# 4. Polynomial features: degree=2 on top-5 from EDA -> X.poly\n",
        "TOP_FEATURES = [\n",
        "    'concave points_worst',\n",
        "    'perimeter_worst',\n",
        "    'concave points_mean',\n",
        "    'radius_worst',\n",
        "    'perimeter_mean',\n",
        "]\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "poly_features = poly.fit_transform(X_processed[TOP_FEATURES])\n",
        "poly_feature_names = poly.get_feature_names_out(TOP_FEATURES)\n",
        "X_poly_generated = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
        "X_poly = pd.concat([\n",
        "    X_processed.drop(columns=TOP_FEATURES).reset_index(drop=True),\n",
        "    X_poly_generated.reset_index(drop=True)\n",
        "], axis=1)\n",
        "print('X_poly:', X_poly.shape)\n",
        "\n",
        "print('\\nTarget y:', y.shape, 'Positives:', int(y.sum()), 'Negatives:', int((1 - y).sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models and cross validation configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class imbalance helper (for XGB): scale_pos_weight = neg/pos\n",
        "neg = int((y == 0).sum())\n",
        "pos = int((y == 1).sum())\n",
        "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
        "\n",
        "models = {\n",
        "    'log_reg': LogisticRegression(solver='liblinear', C=1.0, class_weight='balanced', random_state=SEED),\n",
        "    'linear_svc': LinearSVC(C=1.0, class_weight='balanced', random_state=SEED),\n",
        "    'svc_rbf': SVC(C=1.0, kernel='rbf', gamma='scale', probability=True, random_state=SEED),\n",
        "    'rf': RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=1,\n",
        "        class_weight='balanced_subsample',\n",
        "        n_jobs=-1,\n",
        "        random_state=SEED,\n",
        "    ),\n",
        "    'knn': KNeighborsClassifier(n_neighbors=7, weights='distance'),\n",
        "    'xgb': XGBClassifier(\n",
        "        n_estimators=400,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_lambda=1.0,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='logloss',\n",
        "        random_state=SEED,\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        use_label_encoder=False,\n",
        "    ),\n",
        "}\n",
        "\n",
        "feature_sets = {\n",
        "    'X_processed': X_processed,\n",
        "    'X_pca': X_pca,\n",
        "    'X_with_ratios': X_with_ratios,\n",
        "    'X_poly': X_poly,\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "print('Models:', list(models.keys()))\n",
        "print('Feature sets:', {k: v.shape for k, v in feature_sets.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation helper\n",
        "\n",
        "We'll create a helper function that compute per-fold Recall, F1, Precision, Accuracy, ROC-AUC (handles predict_proba/decision_function).\n",
        "Returns means, stds, and per-fold values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_probas_or_scores(estimator, X):\n",
        "    \"\"\"\n",
        "    Return continuous scores for binary classification metrics.\n",
        "\n",
        "    Prefers estimator.predict_proba(X)[:, 1] if available; falls back to\n",
        "    estimator.decision_function(X); otherwise uses predicted labels (limits AUC).\n",
        "\n",
        "    Args:\n",
        "        estimator: Fitted sklearn-like estimator with predict_proba, decision_function, or predict.\n",
        "        X: Feature matrix (pandas.DataFrame or numpy.ndarray).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[numpy.ndarray, bool]: Scores (probabilities/decision values/labels) and a flag indicating\n",
        "            whether the scores are probabilities (True) or not (False).\n",
        "    \"\"\"\n",
        "    if hasattr(estimator, 'predict_proba'):\n",
        "        proba = estimator.predict_proba(X)[:, 1]\n",
        "        return proba, True\n",
        "    if hasattr(estimator, 'decision_function'):\n",
        "        scores = estimator.decision_function(X)\n",
        "        return scores, False\n",
        "    # Fallback (limited for AUC)\n",
        "    preds = estimator.predict(X)\n",
        "    return preds.astype(float), False\n",
        "\n",
        "\n",
        "def evaluate_model_cv(estimator, X, y, cv):\n",
        "    \"\"\"\n",
        "    Cross-validate a classifier and compute core classification metrics.\n",
        "\n",
        "    Fits the estimator across StratifiedKFold splits and aggregates recall, F1,\n",
        "    precision, accuracy, and ROC-AUC (when scores available).\n",
        "\n",
        "    Args:\n",
        "        estimator: Unfitted sklearn-like classifier (fit will occur inside each fold).\n",
        "        X: Feature matrix (pandas.DataFrame).\n",
        "        y: Binary target (pandas.Series or array-like).\n",
        "        cv: Cross-validator (e.g., StratifiedKFold).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: For each metric, mean, std, and per-fold values:\n",
        "            - 'recall_mean', 'recall_std', 'recall_folds'\n",
        "            - 'f1_mean', 'f1_std', 'f1_folds'\n",
        "            - 'precision_mean', 'precision_std', 'precision_folds'\n",
        "            - 'accuracy_mean', 'accuracy_std', 'accuracy_folds'\n",
        "            - 'roc_auc_mean', 'roc_auc_std', 'roc_auc_folds'\n",
        "    \"\"\"\n",
        "    recalls, f1s, precs, accs, aucs = [], [], [], [], []\n",
        "    \n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        X_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Fit\n",
        "        model = estimator\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        # Predict labels\n",
        "        y_pred = model.predict(X_te)\n",
        "\n",
        "        # Scores for ROC-AUC\n",
        "        scores, is_proba = get_probas_or_scores(model, X_te)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_te, scores)\n",
        "        except Exception:\n",
        "            auc = np.nan\n",
        "\n",
        "        # Metrics\n",
        "        recalls.append(recall_score(y_te, y_pred))\n",
        "        f1s.append(f1_score(y_te, y_pred))\n",
        "        precs.append(precision_score(y_te, y_pred))\n",
        "        accs.append(accuracy_score(y_te, y_pred))\n",
        "        aucs.append(auc)\n",
        "\n",
        "    return {\n",
        "        'recall_mean': np.nanmean(recalls), 'recall_std': np.nanstd(recalls), 'recall_folds': recalls,\n",
        "        'f1_mean': np.nanmean(f1s), 'f1_std': np.nanstd(f1s), 'f1_folds': f1s,\n",
        "        'precision_mean': np.nanmean(precs), 'precision_std': np.nanstd(precs), 'precision_folds': precs,\n",
        "        'accuracy_mean': np.nanmean(accs), 'accuracy_std': np.nanstd(accs), 'accuracy_folds': accs,\n",
        "        'roc_auc_mean': np.nanmean(aucs), 'roc_auc_std': np.nanstd(aucs), 'roc_auc_folds': aucs,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run experiments across models and feature sets\n",
        "\n",
        "results = []\n",
        "raw_folds = []  # store per-fold for plots\n",
        "\n",
        "for fs_name, X_fs in feature_sets.items():\n",
        "    for model_name, estimator in models.items():\n",
        "        metrics = evaluate_model_cv(estimator, X_fs, y, cv)\n",
        "        row = {\n",
        "            'model': model_name,\n",
        "            'feature_set': fs_name,\n",
        "            **{k: v for k, v in metrics.items() if not k.endswith('_folds')}\n",
        "        }\n",
        "        results.append(row)\n",
        "        raw_folds.append({\n",
        "            'model': model_name,\n",
        "            'feature_set': fs_name,\n",
        "            'recall_folds': metrics['recall_folds'],\n",
        "            'f1_folds': metrics['f1_folds'],\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "raw_folds_df = pd.DataFrame(raw_folds)\n",
        "\n",
        "# Sort: Recall desc, then F1 desc\n",
        "results_df = results_df.sort_values(by=['recall_mean', 'f1_mean'], ascending=[False, False]).reset_index(drop=True)\n",
        "\n",
        "os.makedirs(os.path.abspath(os.path.join('..', 'reports', 'metrics')), exist_ok=True)\n",
        "metrics_path = os.path.abspath(os.path.join('..', 'reports', 'metrics', 'model_cv_results.csv'))\n",
        "results_df.to_csv(metrics_path, index=False)\n",
        "\n",
        "print('Top results by Recall:')\n",
        "display(results_df.head(10))\n",
        "print('Saved metrics to: reports/metrics/model_cv_results.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations: boxplots for Recall and F1 by (model, feature_set)\n",
        "\n",
        "os.makedirs(os.path.abspath(os.path.join('..', 'reports', 'figures')), exist_ok=True)\n",
        "fig_dir = os.path.abspath(os.path.join('..', 'reports', 'figures'))\n",
        "\n",
        "# Expand raw_folds for plotting\n",
        "plot_rows = []\n",
        "for row in raw_folds:\n",
        "    for v in row['recall_folds']:\n",
        "        plot_rows.append({'metric': 'recall', 'value': v, 'model': row['model'], 'feature_set': row['feature_set']})\n",
        "    for v in row['f1_folds']:\n",
        "        plot_rows.append({'metric': 'f1', 'value': v, 'model': row['model'], 'feature_set': row['feature_set']})\n",
        "plot_df = pd.DataFrame(plot_rows)\n",
        "\n",
        "# Recall boxplot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=plot_df[plot_df['metric']=='recall'], x='feature_set', y='value', hue='model')\n",
        "plt.title('Recall by Model and Feature Set (5-fold CV)')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Feature Set')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "recall_fig_path = os.path.join(fig_dir, 'model_cv_recall_boxplot.png')\n",
        "plt.tight_layout()\n",
        "# plt.savefig(recall_fig_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# F1 boxplot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=plot_df[plot_df['metric']=='f1'], x='feature_set', y='value', hue='model')\n",
        "plt.title('F1-score by Model and Feature Set (5-fold CV)')\n",
        "plt.ylabel('F1-score')\n",
        "plt.xlabel('Feature Set')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "f1_fig_path = os.path.join(fig_dir, 'model_cv_f1_boxplot.png')\n",
        "plt.tight_layout()\n",
        "# plt.savefig(f1_fig_path, dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print('Saved plots to:', recall_fig_path, 'and', f1_fig_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC/PR curves and confusion matrices for top-2 combos (hold-out split)\n",
        "\n",
        "# Pick top-2 by recall (then F1)\n",
        "top_3 = results_df.head(3)[['model', 'feature_set']].values.tolist()\n",
        "\n",
        "for model_name, fs_name in top_3:\n",
        "    print(f'\\n=== Detailed evaluation: {model_name} on {fs_name} ===')\n",
        "    X_fs = feature_sets[fs_name]\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X_fs, y, test_size=0.25, stratify=y, random_state=SEED)\n",
        "\n",
        "    est = models[model_name]\n",
        "    est.fit(X_tr, y_tr)\n",
        "\n",
        "    # Predictions and scores\n",
        "    y_pred = est.predict(X_te)\n",
        "    scores, is_proba = get_probas_or_scores(est, X_te)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_te, y_pred)\n",
        "    print('Confusion matrix:\\n', cm)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # ROC curve\n",
        "    try:\n",
        "        RocCurveDisplay.from_predictions(y_te, scores, ax=axes[0])\n",
        "        axes[0].set_title(f'ROC Curve: {model_name} on {fs_name}')\n",
        "    except Exception:\n",
        "        axes[0].text(0.5, 0.5, 'ROC curve not available', ha='center', va='center')\n",
        "        axes[0].set_axis_off()\n",
        "\n",
        "    # PR curve\n",
        "    try:\n",
        "        PrecisionRecallDisplay.from_predictions(y_te, scores, ax=axes[1])\n",
        "        axes[1].set_title(f'Precision-Recall Curve: {model_name} on {fs_name}')\n",
        "    except Exception:\n",
        "        axes[1].text(0.5, 0.5, 'PR curve not available', ha='center', va='center')\n",
        "        axes[1].set_axis_off()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # Optional saves:\n",
        "    # fig.savefig(os.path.join(fig_dir, f'roc_pr_{model_name}_{fs_name}.png'), dpi=150)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions & Next Steps\n",
        "\n",
        "- The table above ranks all (model, feature set) combinations by Recall (primary), then F1.\n",
        "- Use the saved CSV and figures for reporting:\n",
        "  - Metrics CSV: `reports/metrics/model_cv_results.csv`\n",
        "  - Figures: `reports/figures/model_cv_*.png`, plus ROC/PR images for top-2\n",
        "\n",
        "Further improvements that could be added:\n",
        "- Hyperparameter tuning (RandomizedSearchCV) for the top-3 combinations.\n",
        "- Threshold tuning on the top model using PR curve to favor Recall while maintaining acceptable Precision.\n",
        "- Interpretability: feature importances (tree models) or SHAP (non-PCA feature sets).\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "breast-cancer-mlops",
      "language": "python",
      "name": "breast-cancer-mlops"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
