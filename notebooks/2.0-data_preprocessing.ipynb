{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 - Data Preprocessing\n",
    "\n",
    "This notebook focuses on the **data preprocessing** stage. The goal is to take the raw data and transform it into a clean, numerical, and scaled format that is ready for machine learning.\n",
    "\n",
    "The scope of this notebook is strictly limited to:\n",
    "1.  Applying initial cleaning steps (using functions from our `.py` scripts).\n",
    "2.  Performing initial feature selection to remove noise.\n",
    "3.  Scaling the features to a standard range.\n",
    "\n",
    "This notebook prepares the data for the next stage, `3.0-feature_engineering.ipynb`, where more advanced techniques like PCA will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import functions from the 'src' folder.\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "# Now we can import our custom functions\n",
    "from model.data_ingestion import load_raw_data\n",
    "from model.data_preprocessing import drop_unnecessary_columns, map_diagnosis_to_numerical, prepare_features_and_target\n",
    "\n",
    "# Display options for pandas\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial cleaning\n",
    "\n",
    "First, we'll load the raw data and apply the pre-existing, tested functions from `src/model/data_preprocessing.py`. This ensures we are reusing our production code and maintaining consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "df_raw = load_raw_data('../data/data.csv')\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply initial cleaning steps from our script\n",
    "df_cleaned = drop_unnecessary_columns(df_raw.copy())\n",
    "df_mapped = map_diagnosis_to_numerical(df_cleaned.copy())\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X, y = prepare_features_and_target(df_mapped)\n",
    "\n",
    "print(\"Shape of features (X) before further preprocessing:\", X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial feature selection (Noise reduction)\n",
    "\n",
    "From the EDA in `1.0-EDA.ipynb`, several features were identified with very low correlation to the target variable (`diagnosis`). These features are more likely to be noise than signal. Removing them is a standard preprocessing step that simplifies the model and potentially improves its performance by reducing noise.\n",
    "\n",
    "**This is distinct** from *feature engineering* or *dimensionality reduction* (like PCA), where the goal is to create new, more informative features. Here, we will simply remove what appears to be irrelevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to drop, identified from EDA (correlation < 0.10 with target)\n",
    "features_to_drop = [\n",
    "    'fractal_dimension_se',\n",
    "    'smoothness_se',\n",
    "    'fractal_dimension_mean',\n",
    "    'texture_se',\n",
    "    'symmetry_se'\n",
    "]\n",
    "\n",
    "# Drop the features\n",
    "X_selected = X.drop(columns=features_to_drop)\n",
    "\n",
    "print(f\"Dropped {len(features_to_drop)} features.\")\n",
    "print(\"Shape of features (X) after selection:\", X_selected.shape)\n",
    "X_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling\n",
    "\n",
    "The features in our dataset have vastly different scales. Machine learning algorithms that use distance calculations (like SVMs or PCA) or gradient-based optimization (like logistic regression) are sensitive to this.\n",
    "\n",
    "We will use `StandardScaler` from scikit-learn to transform the data. It standardizes features by removing the mean and scaling to unit variance. The formula for standardization is:\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "Where:\n",
    "- $ z $ is the scaled value.\n",
    "- $ x $ is the original value.\n",
    "- $ \\mu $ (mu) is the mean of the feature column.\n",
    "- $ \\sigma $ (sigma) is the standard deviation of the feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "X_scaled_array = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame for better readability\n",
    "X_processed = pd.DataFrame(X_scaled_array, columns=X_selected.columns)\n",
    "\n",
    "print(\"Data successfully scaled.\")\n",
    "X_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the summary statistics of the processed data to confirm that it has been standardized (It should show a mean close to 0 and a standard deviation close to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next steps\n",
    "\n",
    "The data preprocessing step is complete. We have created a fully preprocessed feature set (`X_processed`) and have our target variable (`y`).\n",
    "\n",
    "**Boundary:**\n",
    "- The output of this notebook is the clean and scaled data.\n",
    "- This data is the direct input for the `3.0-feature_engineering.ipynb` notebook, where we'll explore techniques like Principal Component Analysis (PCA) to create new, engineered features from this processed dataset. We will also investigate other feature creation methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast-cancer-mlops",
   "language": "python",
   "name": "breast-cancer-mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
