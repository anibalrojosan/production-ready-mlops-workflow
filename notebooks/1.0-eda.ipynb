{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58aa81a0",
   "metadata": {},
   "source": [
    "# Exploratory data analysis of Breast Cancer Wisconsin Diagnostic data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954df55a",
   "metadata": {},
   "source": [
    "This notebook conducts an Exploratory Data Analysis (EDA) on the Breast Cancer Wisconsin (Diagnostic) dataset. \n",
    "\n",
    "The primary goals of this EDA are to:\n",
    "1. Understand the structure and characteristics of the data.\n",
    "2. Visualize the distributions of key features.\n",
    "3. Analyze the relationships and correlations between different variables.\n",
    "4. Identify potential data quality issues, such as missing values or outliers.\n",
    "5. Form initial hypotheses that will guide the subsequent data preprocessing and feature engineering stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e337cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.float_format', '{:f}'.format)\n",
    "\n",
    "# Set some plotting styles for better visuals\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the path to save figures\n",
    "FIGURES_PATH = '../reports/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd613db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/data.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693ae82",
   "metadata": {},
   "source": [
    "## 1. Initial data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataframe info:\")\n",
    "df.info()\n",
    "\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a933075",
   "metadata": {},
   "source": [
    "Column \"Unnamed: 32\" has 569 missing values, which represents the 100% of the data, so it can be dropped. Column \"id\" is an identifier and does not provide any information about the patient, so it can be dropped too.\n",
    "\n",
    "Column \"diagnosis\" is the target variable. I'll map the values to 0 and 1 for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f55772",
   "metadata": {},
   "source": [
    "## 2. Target variable analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.countplot(x='diagnosis', data=df)\n",
    "\n",
    "# Add counts on top of the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'center', xytext = (0, 9), textcoords = 'offset points')\n",
    "\n",
    "plt.title('Distribution of diagnosis (M = Malignant, B = Benign)')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(f'{FIGURES_PATH}diagnosis_distribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Proportion of malignant and benign cases\n",
    "print(df['diagnosis'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b5701",
   "metadata": {},
   "source": [
    "The target variable is imbalanced, but not severely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585f9c7",
   "metadata": {},
   "source": [
    "## 3. Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # Just to show all columns\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64277ae",
   "metadata": {},
   "source": [
    "There are great differences in the scale of the features, so a feature scaling technique is needed. I'll use StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c384c5f",
   "metadata": {},
   "source": [
    "## 4. Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8293b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I'll map the diagnosis to a numerical format for correlation calculation\n",
    "df_corr = df.copy()\n",
    "df_corr['diagnosis'] = df_corr['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Drop the ID and the empty column\n",
    "df_corr = df_corr.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "\n",
    "corr_matrix = df_corr.corr()\n",
    "\n",
    "plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False) # annot=False because there are too many features\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.savefig(f'{FIGURES_PATH}correlation_matrix.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aaab01",
   "metadata": {},
   "source": [
    "There's a strong correlation between features involving radius, perimeter and area of tumors, which is expected. Let's see the features with the strongest correlation with 'radius_mean'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_with_radius_mean = corr_matrix['radius_mean']\n",
    "\n",
    "# Calculate the absolute value of the correlations to measure the strength\n",
    "abs_corr_with_radius_mean = corr_with_radius_mean.abs()\n",
    "\n",
    "# Sort the absolute correlations in descending order (from highest to lowest)\n",
    "most_correlated_features_radius_mean = abs_corr_with_radius_mean.sort_values(ascending=False)\n",
    "\n",
    "print(\"Features sorted by it's correlation with 'radius_mean':\")\n",
    "display(most_correlated_features_radius_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375b587",
   "metadata": {},
   "source": [
    "The features with the strongest correlation with 'radius_mean' are 'perimeter_mean' and 'area_mean'. This is expected because the perimeter and area of a circle are directly proportional to its radius. This also suggests that this features are redundant and good candidates for feature reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95e368",
   "metadata": {},
   "source": [
    "Let's look at the features with the strongest correlation with 'diagnosis'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d48cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_with_diagnosis = corr_matrix['diagnosis']\n",
    "\n",
    "# Calculate the absolute value of the correlations to measure the strength\n",
    "abs_corr_with_diagnosis = corr_with_diagnosis.abs()\n",
    "\n",
    "# Sort the absolute correlations in descending order (from highest to lowest)\n",
    "most_correlated_features_diagnosis = abs_corr_with_diagnosis.sort_values(ascending=False)\n",
    "\n",
    "print(\"Features sorted by it's correlation with 'diagnosis':\")\n",
    "display(most_correlated_features_diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c96002",
   "metadata": {},
   "source": [
    "The last 5 features (correlation < 0.10), mostly add noise to the model, they are also candidates for feature reduction.\n",
    "\n",
    "Features to be dropped: I'll drop the features with low correlation with the target variable, but keep the ones highly correlated with each other, like 'radius_mean' and 'perimeter_mean'. These will be handled by PCA.\n",
    "\n",
    "So, the features to be dropped are:\n",
    "- fractal_dimension_se      \n",
    "- smoothness_se             \n",
    "- fractal_dimension_mean    \n",
    "- texture_se                \n",
    "- symmetry_se               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b552a3",
   "metadata": {},
   "source": [
    "## 5. Feature distributions by diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.copy()\n",
    "df_corr['diagnosis'] = df_corr['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Drop non-numeric or irrelevant columns\n",
    "if 'id' in df_corr.columns:\n",
    "    df_corr = df_corr.drop('id', axis=1)\n",
    "if 'Unnamed: 32' in df_corr.columns:\n",
    "    df_corr = df_corr.drop('Unnamed: 32', axis=1)\n",
    "\n",
    "# Calculate most correlated features with 'diagnosis'\n",
    "corr_with_target = df_corr.corr()['diagnosis'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Select top 5 features\n",
    "top_5_features = corr_with_target.index[1:6] # First index is 'diagnosis'\n",
    "print(\"Top 5 most correlated features with diagnosis:\")\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea66034",
   "metadata": {},
   "source": [
    "From the EDA, we know that this features with very different scales, so we'll separate them into small scale and large scale for better plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f340514",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_large_scale = ['perimeter_worst', 'radius_worst', 'perimeter_mean']\n",
    "features_small_scale = ['concave points_worst', 'concave points_mean']\n",
    "\n",
    "print(\"Plotting features with large scales:\", features_large_scale)\n",
    "print(\"Plotting features with small scales:\", features_small_scale)\n",
    "\n",
    "\n",
    "df_melted_large = pd.melt(df, \n",
    "                          id_vars=\"diagnosis\", \n",
    "                          value_vars=features_large_scale,\n",
    "                          var_name=\"feature\", \n",
    "                          value_name=\"value\")\n",
    "\n",
    "df_melted_small = pd.melt(df, \n",
    "                          id_vars=\"diagnosis\", \n",
    "                          value_vars=features_small_scale,\n",
    "                          var_name=\"feature\", \n",
    "                          value_name=\"value\")\n",
    "\n",
    "# We create a figure with 2 rows and 1 column of plots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "fig.suptitle('Distribution of Top Features by Diagnosis (Separated by Scale)', fontsize=18)\n",
    "\n",
    "# Plot 1: Large Scale Features\n",
    "sns.boxplot(ax=axes[0], x=\"feature\", y=\"value\", hue=\"diagnosis\", data=df_melted_large, palette=\"viridis\")\n",
    "axes[0].set_title('Features with Larger Value Ranges', fontsize=14)\n",
    "axes[0].set_xlabel('') # Remove x-axis label for the top plot for cleanliness\n",
    "axes[0].set_ylabel('Value', fontsize=12)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot 2: Small Scale Features\n",
    "sns.boxplot(ax=axes[1], x=\"feature\", y=\"value\", hue=\"diagnosis\", data=df_melted_small, palette=\"plasma\")\n",
    "axes[1].set_title('Features with Smaller Value Ranges', fontsize=14)\n",
    "axes[1].set_xlabel('Feature', fontsize=12)\n",
    "axes[1].set_ylabel('Value', fontsize=12)\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust layout to make room for the suptitle\n",
    "plt.savefig(f'{FIGURES_PATH}feature_distribution_with_target_boxplots.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0b05e",
   "metadata": {},
   "source": [
    "Let's also create a pairplot of the top 5 figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7778c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features_with_diagnosis = top_5_features.tolist() + ['diagnosis']\n",
    "df_pairplot = df[top_5_features_with_diagnosis]\n",
    "\n",
    "print(\"Generating pair plot for top 5 features. This may take a moment...\")\n",
    "\n",
    "# The 'hue' parameter colors the data points by the 'diagnosis' column\n",
    "sns.pairplot(df_pairplot, hue='diagnosis', palette='viridis', diag_kind='kde')\n",
    "plt.savefig(f'{FIGURES_PATH}top_features_pairplot.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ba855",
   "metadata": {},
   "source": [
    "### Key observations from feature distribution plots\n",
    "\n",
    "1.  **All these features are highly predictive:** Every feature visualized shows a clear and significant difference in its value distribution between Malignant (M) and Benign (B) tumors, confirming their high predictive power.\n",
    "\n",
    "2.  **Malignant tumors are consistently larger:** Size-related features like `perimeter_worst` and `radius_worst` are consistently higher for malignant cases, with minimal overlap between the two diagnostic groups.\n",
    "\n",
    "3.  **Concave points are a critical differentiator:** The `concave points` features show a dramatic separation between classes, with malignant tumors having substantially higher values. This makes them one of the most important indicators.\n",
    "\n",
    "4.  **High confidence in model separability:** The distinct separation of distributions suggests that even a simple machine learning model will be able to effectively learn a boundary to distinguish between malignant and benign cases with high accuracy.\n",
    "\n",
    "5.  **Visual confirmation of redundancy:** The fact that all size-related features tell the same story (malignant bigger than benign) visually confirms they are measuring a similar underlying concept. This further suggests the use of PCA to handle multicollinearity in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea795891",
   "metadata": {},
   "source": [
    "## 6. Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all numeric features\n",
    "features = df_corr.select_dtypes(include='number').drop('diagnosis', axis=1, errors='ignore')\n",
    "diagnosis = df_corr['diagnosis']\n",
    "\n",
    "features_to_drop = [\n",
    "    'fractal_dimension_se',\n",
    "    'smoothness_se',\n",
    "    'fractal_dimension_mean',\n",
    "    'texture_se',\n",
    "    'symmetry_se'\n",
    "]\n",
    "print(f\"Dropping {len(features_to_drop)} noisy features before PCA.\")\n",
    "features_cleaned = features.drop(columns=features_to_drop)\n",
    "\n",
    "# Scale the cleaned features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_cleaned)\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Create a new DataFrame with the principal components\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC_1', 'PC_2'])\n",
    "df_pca['diagnosis'] = diagnosis\n",
    "\n",
    "print(f\"Variance explained by PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"Variance explained by PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"Total variance explained by first two components: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='PC_1', y='PC_2', hue='diagnosis', data=df_pca, alpha=0.8, palette='magma')\n",
    "\n",
    "plt.title('2D PCA Projection of Breast Cancer Data (Noise Removed)', fontsize=16)\n",
    "plt.xlabel('Principal Component 1', fontsize=12)\n",
    "plt.ylabel('Principal Component 2', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(f'{FIGURES_PATH}pca_2d_projection.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd359cc",
   "metadata": {},
   "source": [
    "Let's add an extra principal component to add more captured variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c918cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_corr.select_dtypes(include='number').drop('diagnosis', axis=1, errors='ignore')\n",
    "diagnosis = df_corr['diagnosis']\n",
    "\n",
    "features_to_drop = [\n",
    "    'fractal_dimension_se',\n",
    "    'smoothness_se',\n",
    "    'fractal_dimension_mean',\n",
    "    'texture_se',\n",
    "    'symmetry_se'\n",
    "]\n",
    "print(f\"Dropping {len(features_to_drop)} noisy features before PCA.\")\n",
    "features_cleaned = features.drop(columns=features_to_drop)\n",
    "\n",
    "# Scale the cleaned features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_cleaned)\n",
    "\n",
    "# Apply PCA to reduce to 3 dimensions\n",
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Create a new DataFrame with the three principal components\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC_1', 'PC_2', 'PC_3'])\n",
    "df_pca['diagnosis'] = diagnosis\n",
    "\n",
    "print(f\"Variance explained by PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"Variance explained by PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"Variance explained by PC3: {pca.explained_variance_ratio_[2]:.2%}\")\n",
    "print(f\"Total variance explained by first three components: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Plot the results in 3D\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Separate data for plotting\n",
    "df_m = df_pca[df_pca['diagnosis'] == 1]\n",
    "df_b = df_pca[df_pca['diagnosis'] == 0]\n",
    "\n",
    "# Plot each class separately\n",
    "ax.scatter(df_m['PC_1'], df_m['PC_2'], df_m['PC_3'], c='purple', label='Malignant', alpha=0.6)\n",
    "ax.scatter(df_b['PC_1'], df_b['PC_2'], df_b['PC_3'], c='orange', label='Benign', alpha=0.6)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_title('3D PCA Projection of Breast Cancer Data (Noise Removed)', fontsize=16)\n",
    "ax.set_xlabel('Principal Component 1', fontsize=12)\n",
    "ax.set_ylabel('Principal Component 2', fontsize=12)\n",
    "ax.set_zlabel('Principal Component 3', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.savefig(f'{FIGURES_PATH}pca_3d_projection.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b137d0",
   "metadata": {},
   "source": [
    "Let's determine how much PC are needed to capture ~90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA().fit(features_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca_full.explained_variance_ratio_), \n",
    "         marker='o', \n",
    "         linestyle='--')\n",
    "\n",
    "plt.title('Cumulative Explained Variance by Number of Components', fontsize=16)\n",
    "plt.xlabel('Number of Components', fontsize=12)\n",
    "plt.ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0.90, color='r', linestyle=':', label='90% Explained Variance')\n",
    "plt.legend()\n",
    "plt.savefig(f'{FIGURES_PATH}pca_cum_variance_by_n_of_components.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7523aec",
   "metadata": {},
   "source": [
    "### Key observations from PCA\n",
    "\n",
    "1.  **High redundancy confirmed:** The steep initial curve of the explained variance plot confirms that the original features are highly correlated.\n",
    "\n",
    "2.  **Optimal component range identified:** The \"elbow\" of the curve occurs between 2 and 3 components, thus indicating that the majority of the crucial information is captured within these first few components.\n",
    "\n",
    "3.  **Data-Driven feature reduction:** The analysis provides a clear strategy for selecting the number of components. Approximately **7 components** are required to capture over 90% of the total variance, offering a robust trade-off between information retention and model simplicity.\n",
    "\n",
    "4.  **Excellent class separability:** The 2D and 3D projections show that the data points form two distinct, well-separated clusters corresponding to the Malignant and Benign diagnoses. This gives high confidence that a machine learning model can perform well on this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5e7d5",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "This Exploratory Data Analysis has provided several key insights:\n",
    "- The dataset is clean, with the only missing values being in a completely empty column (`Unnamed: 32`) that can be dropped.\n",
    "- The target variable is slightly imbalanced, but not severely.\n",
    "- Many features are highly correlated, confirming that dimensionality reduction techniques like PCA are appropriate.\n",
    "- A clear separation between classes is visible, suggesting that machine learning models will perform well.\n",
    "- We have identified a set of 5 features with very low correlation to the target, which can be dropped during preprocessing to reduce noise.\n",
    "\n",
    "The next logical step is to formally preprocess this data to prepare it for modeling.\n",
    "\n",
    "**Next Notebook:** [**2.0-data_preprocessing.ipynb**](./2.0-data_preprocessing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast-cancer-mlops",
   "language": "python",
   "name": "breast-cancer-mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
