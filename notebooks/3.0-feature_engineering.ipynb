{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.0 - Feature engineering\n",
        "\n",
        "This notebook focuses on **feature engineering**. The goal is to use the clean, preprocessed data from the previous stage to create new, potentially more informative features using different techniques.\n",
        "\n",
        "We will explore three main approaches:\n",
        "1.  **Principal Component Analysis (PCA):** A dimensionality reduction technique to address multicollinearity.\n",
        "2.  **Ratio features:** Creating new features based on domain knowledge to capture geometric properties like shape and growth.\n",
        "3.  **Polynomial features:** Automatically generating interaction and higher-order features to capture non-linear relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and preprocess data\n",
        "\n",
        "As a best practice for reproducibility, we will start by running the complete preprocessing pipeline developed in the previous notebooks. This provides a self-contained environment and gives us the clean, scaled dataset that is the starting point for all our feature engineering experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the src directory to the Python path\n",
        "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
        "\n",
        "# Import custom functions\n",
        "from model.data_ingestion import load_raw_data\n",
        "from model.data_preprocessing import drop_unnecessary_columns, map_diagnosis_to_numerical, prepare_features_and_target\n",
        "\n",
        "# Import necessary tools from scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Set plotting styles\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "# Display options for pandas\n",
        "pd.set_option('display.max_columns', None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by running the complete preprocessing pipeline that was developed and tested in `2.0-data_preprocessing.ipynb`. This will provide us with the clean, scaled dataset that serves as the starting point for our feature engineering work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "df_raw = load_raw_data('../data/data.csv')\n",
        "\n",
        "# Initial Cleaning & Mapping\n",
        "df_cleaned = drop_unnecessary_columns(df_raw.copy())\n",
        "df_mapped = map_diagnosis_to_numerical(df_cleaned.copy())\n",
        "\n",
        "# We need the original, unscaled features to create meaningful ratios\n",
        "X_unscaled, y = prepare_features_and_target(df_mapped)\n",
        "\n",
        "# Initial Feature Selection\n",
        "features_to_drop = [\n",
        "    'fractal_dimension_se',\n",
        "    'smoothness_se',\n",
        "    'fractal_dimension_mean',\n",
        "    'texture_se',\n",
        "    'symmetry_se'\n",
        "]\n",
        "\n",
        "X_selected_unscaled = X_unscaled.drop(columns=features_to_drop)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled_array = scaler.fit_transform(X_selected_unscaled)\n",
        "X_processed = pd.DataFrame(X_scaled_array, columns=X_selected_unscaled.columns)\n",
        "\n",
        "print(\"Data successfully preprocessed.\")\n",
        "print(\"Shape of baseline processed features (X_processed):\", X_processed.shape)\n",
        "X_processed.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Technique 1: Principal Component Analysis (PCA)\n",
        "\n",
        "First, we'll apply PCA to our baseline `X_processed` dataset to create a lower-dimensional feature set. PCA is a dimensionality reduction technique that transforms a large set of correlated variables into a smaller set of uncorrelated variables called **principal components**.\n",
        "\n",
        "This principal components are:\n",
        "- linear combinations of the original features,\n",
        "- ordered by the amount of variance they explain in the data.\n",
        "\n",
        "This is ideal for our dataset that showed high multicollinearity, especially among features related to size of tumors (radius, perimeter, and area). By using PCA, we can reduce redundancy and create a more compact and efficient feature set for our models.\n",
        "\n",
        "A disadvantage of PCA is that the principal components are not interpretable, which can make it difficult to understand the underlying structure of the data, making the model less interpretable and harder to explain to stakeholders. We always need to aim for a model with sufficient performance that can generalize well to new data, but also take into account its interpretability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine optimal number of components\n",
        "pca_full = PCA().fit(X_processed)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
        "         np.cumsum(pca_full.explained_variance_ratio_), \n",
        "         marker='o', linestyle='--')\n",
        "plt.title('Cumulative Explained Variance by Number of Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.90, color='r', linestyle=':', label='90% Explained Variance')\n",
        "plt.axhline(y=0.95, color='g', linestyle=':', label='95% Explained Variance')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA with 8 components to capture >95% of variance\n",
        "pca = PCA(n_components=8)\n",
        "X_pca_array = pca.fit_transform(X_processed)\n",
        "pca_columns = [f'PC_{i+1}' for i in range(X_pca_array.shape[1])]\n",
        "X_pca = pd.DataFrame(X_pca_array, columns=pca_columns)\n",
        "\n",
        "print(\"Shape of PCA features (X_pca):\", X_pca.shape)\n",
        "X_pca.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Technique 2: Ratio Features (Shape Metrics)\n",
        "\n",
        "Now, let's create features based on domain knowledge. We'll calculate ratios that represent tumor shape and growth as we know that these are features that can be used to predict the diagnosis. These must be calculated on the **unscaled** data to preserve their physical meaning. Afterwards, we will scale the complete dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the unscaled data with selected features\n",
        "X_with_ratios_unscaled = X_selected_unscaled.copy()\n",
        "\n",
        "# Create shape factor\n",
        "# Using original feature names from before selection\n",
        "X_with_ratios_unscaled['shape_factor'] = X_unscaled['perimeter_mean']**2 / X_unscaled['area_mean']\n",
        "\n",
        "# Create growth factor\n",
        "X_with_ratios_unscaled['radius_growth'] = X_unscaled['radius_worst'] / X_unscaled['radius_mean']\n",
        "\n",
        "# Scale the entire new feature set\n",
        "scaler_ratios = StandardScaler()\n",
        "X_ratios_scaled_array = scaler_ratios.fit_transform(X_with_ratios_unscaled)\n",
        "X_with_ratios = pd.DataFrame(X_ratios_scaled_array, columns=X_with_ratios_unscaled.columns)\n",
        "\n",
        "print(\"Shape of features with ratios (X_with_ratios):\", X_with_ratios.shape)\n",
        "X_with_ratios.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Technique 3: Polynomial Features\n",
        "\n",
        "This technique automatically creates interaction terms and higher-order features. To avoid creating too many features (the \"curse of dimensionality\"), we will only apply this to the top 5 most predictive features identified in our EDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top 5 features from EDA\n",
        "top_features = [\n",
        "    'concave points_worst', \n",
        "    'perimeter_worst', \n",
        "    'concave points_mean',\n",
        "    'radius_worst', \n",
        "    'perimeter_mean'\n",
        "]\n",
        "\n",
        "# Initialize PolynomialFeature, degree=2 will create interaction terms (a*b) and quadratic terms (a^2)\n",
        "# include_bias=False prevents adding a constant column of ones\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# Fit and transform the top features from the SCALED data\n",
        "poly_features = poly.fit_transform(X_processed[top_features])\n",
        "\n",
        "# Create a DataFrame with the new polynomial feature names\n",
        "poly_feature_names = poly.get_feature_names_out(top_features)\n",
        "X_poly_generated = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
        "\n",
        "# Drop the original top 5 features from our main processed set to avoid duplication\n",
        "X_processed_without_top5 = X_processed.drop(columns=top_features)\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "X_poly = pd.concat([X_processed_without_top5.reset_index(drop=True), X_poly_generated.reset_index(drop=True)], axis=1)\n",
        "\n",
        "print(\"Shape of polynomial features (X_poly):\", X_poly.shape)\n",
        "X_poly.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, a lot of new features are created, but we can see that the number of features is still manageable. This added complexity will be useful to capture non-linear relationships in the data, but we need to be careful not to overfit the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Next steps\n",
        "\n",
        "We have engineered several new feature sets, giving us multiple candidates for our modeling stage.\n",
        "\n",
        "**Generated Feature Sets:**\n",
        "1.  **`X_processed`**: Baseline cleaned and scaled features (25 features).\n",
        "2.  **`X_pca`**: PCA-transformed features (8 features).\n",
        "3.  **`X_with_ratios`**: Baseline features plus our new shape and growth metrics, all scaled (27 features).\n",
        "4.  **`X_poly`**: Baseline features with the top 5 replaced by their polynomial/interaction terms (40 features).\n",
        "\n",
        "**Next Notebook:** `4.0-model_experimentation.ipynb`. In this next step, we will train and evaluate various classification models on all four of these feature sets to find the combination that yields the best performance.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "breast-cancer-mlops",
      "language": "python",
      "name": "breast-cancer-mlops"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
